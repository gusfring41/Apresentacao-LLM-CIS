{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899ea6a3",
   "metadata": {},
   "source": [
    "# Passo a passo de Transformadores\n",
    "\n",
    "A arquitetura usada foi a implementada originalmente no repositório [_Vanilla Transformer_](https://github.com/arxyzan/vanilla-transformer) com autoria de [Aryan Shekarlaban](https://github.com/arxyzan). A arquitetura, como descrito pelo autor, almeja fazer uma implementação simples e fiel à arquitetura proposta pelo artigo original [_Attention is all you need_](../../papers/attention_is_all_you_need.pdf).\n",
    "\n",
    "<center>\n",
    "<img src='assets/transformers_architecture.png' height=400/>\n",
    "</center>\n",
    "\n",
    "O código possui 2 modelos, um transformador completo na classe `Transformer`,e um classificador utilizando apenas o _encoder_ na classe `EncoderClassifier`. Além disso possui 2 \"blocos\", um _encoder_ na classe `Encoder`, um _decoder_ na classe `Decoder`. Finalmente possui 3 tipos de camada, a camada de _auto-atenção_ na classe `MultiHeadAttention`, a camada de _encoder_ na classe `EncoderLayer`, e a camada de _decoder_ na classe `Decoder`. Abaixo as classes são descritas com maior detalhe.\n",
    "\n",
    "Esse notebook descreve passo a passo o código usado para implementar transformadores com PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bce8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando pacotes\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600be35",
   "metadata": {},
   "source": [
    "## Camadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1bbd8",
   "metadata": {},
   "source": [
    "### `MultiHeadAttention`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f06f6",
   "metadata": {},
   "source": [
    "A implementação da classe segue a seguinte arquitetura:\n",
    "\n",
    "<center>\n",
    "<img src='assets/multihead_attention.png' height=300/>\n",
    "</center>\n",
    "\n",
    "Essa classe implementa uma camada de _Multi-Head Self Attention_, recebe como entrada 3 parâmetros:\n",
    "\n",
    "- `embed_dim`: Representa a dimensão total do embedding de cada token, que nada mais é do que o tamanho do vetor de características para cada token após a camada de embedding.\n",
    "\n",
    "- `n_heads`: Representa o número de cabeças de atenção. Cada _head_ é um mecanismo de atenção independente que processa diferentes subespaços de uma mesma entrada. Isso acontece devido a diferentes pesos em cada _head_.\n",
    "\n",
    "- `dropout`: probabilidade _p_ de zerar um elemento do vetor de entrada.\n",
    "\n",
    "O algoritmo de atenção é descrito pela imagem abaixo:\n",
    "\n",
    "<center>\n",
    "<img src='assets/attention.png' height=300/>\n",
    "</center>\n",
    "\n",
    "#### Projeção linear\n",
    "\n",
    "O primeiro passo no método `forward` é fazer projeções lineares dos vetores de _queries, keys_ e _values_, de modo a permitir que o modelo aprenda diferentes representações para cada um dos papeis em uma mesma entrada. Essa etapa não altera a dimensão dos vetores.\n",
    "\n",
    "```python\n",
    "        # [batch_size, seq_len, embed_dim] -> [batch_size, seq_len, embed_dim]\n",
    "        Q = self.queries(q)\n",
    "        K = self.keys(k)\n",
    "        V = self.values(v)\n",
    "```\n",
    "\n",
    "#### Redimensionamento e separação de _heads_\n",
    "\n",
    "Em seguida é feita a preparação para o algoritmo de atenção, com o redimensionamento dos tensores `Q`, `K` e `V` para separação das _heads_. Os tensores são redimensionados, em um primeiro momento, de $(batch\\ size \\times seq\\ len \\times embed\\ dim)$ para $(batch\\ size \\times seq\\ len \\times embed\\ dim \\times head\\ dim)$. Esse redimensionamento é feito pelo método `.view`.\n",
    "\n",
    "Após o redimensionamento, é feita uma permutação das dimensões de modo que o número de cabeças seja a segunda dimensão. Isso mais uma vez muda as dimensões do vetor, dessa vez de $(batch\\ size \\times head\\ dim \\times seq\\ len \\times head\\ dim)$. A permutação é feita pelo método `.permute`\n",
    "\n",
    "```python\n",
    "        # [batch_size, seq_len, embed_dim] -> [batch_size, head_dim, seq_len, head_dim]\n",
    "        Q = Q.view(N, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(N, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(N, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "```\n",
    "\n",
    "#### Queries X Keys e normalização\n",
    "\n",
    "Feito o redimensionamento, o próximo passo é fazer a multiplicação das _queries_ com as _keys_. Para multiplicar esses dois tensores, é necessário primeiro fazer um redimensionamento do tensor de _keys_, botando a dimensão $head\\ dim$ no eixo $2$ ao invés do eixo $3$.\n",
    "\n",
    "Isso é feito de modo a fazer produto escalar do vetor de embedding de cada token (para cada cabeça na dimensão $head\\ dim$ das _queries_), com o vetor de embedding (também para cada cabeça na dimensão $head\\ dim$ das _keys_) de todos os tokens da sequência. Ou seja, para cada token da _query_, calculamos a similaridade com cada token da _key_ ao longo de uma sequência.\n",
    "\n",
    "Essa multiplicação, chamada _energia_ (_energy_), é então normalizada pela raiz quadrada da $embed\\ dim$, ajudando a estabilizar o gradiente ao longo do treinamento.\n",
    "\n",
    "```python\n",
    "        # [batch_size, head_dim, seq_len, head_dim] -> [batch_size, n_heads, seq_len, seq_len]]\n",
    "        energy = (Q.matmul(K.permute(0, 1, 3, 2))) / self.scale\n",
    "```\n",
    "\n",
    "#### Aplicação de máscara e softmax\n",
    "\n",
    "A máscara controla quais posições da sequência o modelo pode ou não considerar durante o cálculo da atenção. Nesse caso a ideia é remover a influência de tokens de padding adicionados à sequência. Isso é feito com o método `.masked_fill`, substituindo valores de $0$ por valores extremamente pequenos $-1\\times10^{20}$, de modo que após a passagem pela função _softmax_ a probabilidade correspondente seja praticamente zero.\n",
    "\n",
    "A função _softmax_ é aplicada ao longo da última dimensão do tensor, de modo a transformar os scores de atenção (`energy`) em probabilidades.\n",
    "\n",
    "```python\n",
    "        # [batch_size, n_heads, seq_len, seq_len]\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e20)\n",
    "\n",
    "        attention = energy.softmax(-1)\n",
    "```\n",
    "\n",
    "#### Attention X Values\n",
    "\n",
    "A multiplicação entre os valores de _atenção_ (_attention_) e _values_ mensura quanto os tokens devem prestar atenção uns aos outros. Essa multiplicação é feita juntamente a um dropout nos vetores de atenção.\n",
    "\n",
    "```python\n",
    "        # [batch_size, n_heads, seq_len, seq_len]\n",
    "        x = self.dropout(attention).matmul(V)\n",
    "```\n",
    "\n",
    "#### Redimensionamento e concatenação\n",
    "\n",
    "Novamente os tensores passam por um redimensionamento, devolvendo a dimensão $head\\ dim$ ao final do tensor. O método `.contiguous` garante que o tensor seja armazenado de forma contínua, para as operações subsequentes. Após o redimensionamento, as _heads_ são concatenadas, achatando a segunda e última dimensão, com todas as representações produzidas por cada uma das _heads_ colocadas lado a lado, formando um único vetor de dimensão $embed\\ dim$.\n",
    "\n",
    "Após a concatenação, o tensor passa por uma outra camada linear, de modo a \"misturar\" as informações das diferentes heads e projetar o resultado de volta ao espaço de embedding original.\n",
    "\n",
    "```python\n",
    "        # [batch_size, n_heads, seq_len, seq_len] -> [batch_size, seq_len, n_heads, head_dim]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(N, -1, self.embed_dim)\n",
    "        x = self.proj(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2211b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Self-attention layer for Transformer.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Class initializer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary.\n",
    "            n_layers: Number of Encoder layers.\n",
    "            n_classes: Number of classes for output.\n",
    "            ff_hid_dim: Size of hidden dimension in feed-forward layer.\n",
    "            embed_dim: Size of embedding dimension.\n",
    "            n_heads: Number of self-attention heads.\n",
    "            max_length: Maximum length of vector.\n",
    "            pad_idx: Index of padding token.\n",
    "            dropout: p of dropout.\n",
    "            device: Computing device\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.scale = embed_dim**0.5\n",
    "\n",
    "        self.keys = nn.Linear(embed_dim, embed_dim)\n",
    "        self.queries = nn.Linear(embed_dim, embed_dim)\n",
    "        self.values = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through architecture.\n",
    "\n",
    "        Args:\n",
    "            q: Tensor of queries.\n",
    "            k: Tensor of keys.\n",
    "            v: Tensor of values.\n",
    "            mask: Energy mask.\n",
    "        '''\n",
    "        N = q.shape[0]\n",
    "\n",
    "        Q = self.queries(q)\n",
    "        K = self.keys(k)\n",
    "        V = self.values(v)\n",
    "\n",
    "        Q = Q.view(N, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(N, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(N, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        energy = (Q.matmul(K.permute(0, 1, 3, 2))) / self.scale\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e20)\n",
    "\n",
    "        attention = energy.softmax(-1)\n",
    "\n",
    "        x = self.dropout(attention).matmul(V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(N, -1, self.embed_dim)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbbb7b",
   "metadata": {},
   "source": [
    "### `EncoderLayer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a795e",
   "metadata": {},
   "source": [
    "A classe `EncoderLayer` segue a arquitetura abaixo:\n",
    "\n",
    "<center>\n",
    "<img src='assets/encoder_layer.png' height=300/>\n",
    "</center>\n",
    "\n",
    "A camada recebe 4 parâmetros como entrada:\n",
    "\n",
    "- `embed_dim`: Representa a dimensão total do embedding de cada token, que nada mais é do que o tamanho do vetor de características para cada token após a camada de embedding.\n",
    "\n",
    "- `n_heads`: Representa o número de cabeças de atenção. Cada _head_ é um mecanismo de atenção independente que processa diferentes subespaços de uma mesma entrada. Isso acontece devido a diferentes pesos em cada _head_.\n",
    "\n",
    "- `ff_hid_dim`: Representa o número de unidades da camada oculta dentro do bloco feed-forward do _encoder_.\n",
    "\n",
    "- `dropout`: Probabilidade _p_ de zerar um elemento do vetor de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18689902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Encoder layer with self-attention.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, embed_dim: int, n_heads: int, ff_hid_dim: int, dropout: float\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Class initializer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Size of embedding dimension.\n",
    "            n_heads: Number of self-attention heads.\n",
    "            ff_hid_dim: Size of hidden dimension in feed-forward layer.\n",
    "            device: Computing device\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim, n_heads=n_heads, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm_2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_hid_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through architecture.\n",
    "\n",
    "        Args:\n",
    "            src: Source tensor.\n",
    "            mask: Source tensor mask.\n",
    "        '''\n",
    "        attention = self.attention(src, src, src, mask)\n",
    "        x = self.norm_1(attention + self.dropout(src))\n",
    "\n",
    "        out = self.mlp(x)\n",
    "        out = self.norm_2(out + self.dropout(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f155c",
   "metadata": {},
   "source": [
    "### `DecoderLayer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd3aed",
   "metadata": {},
   "source": [
    "A classe `DecoderLayer` segue a arquitetura abaixo:\n",
    "\n",
    "<center>\n",
    "<img src='assets/decoder_layer.png' height=300/>\n",
    "</center>\n",
    "\n",
    "A camada recebe 4 parâmetros como entrada:\n",
    "\n",
    "- `embed_dim`: Representa a dimensão total do embedding de cada token, que nada mais é do que o tamanho do vetor de características para cada token após a camada de embedding.\n",
    "\n",
    "- `n_heads`: Representa o número de cabeças de atenção. Cada _head_ é um mecanismo de atenção independente que processa diferentes subespaços de uma mesma entrada. Isso acontece devido a diferentes pesos em cada _head_.\n",
    "\n",
    "- `ff_hid_dim`: Representa o número de unidades da camada oculta dentro do bloco feed-forward do _decoder_.\n",
    "\n",
    "- `dropout`: Probabilidade _p_ de zerar um elemento do vetor de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3302b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Decoder layer with self-attention.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        n_heads: int,\n",
    "        ff_hid_dim: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Class initializer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Size of embedding dimension.\n",
    "            n_heads: Number of self-attention heads.\n",
    "            ff_hid_dim: Size of hidden dimension in feed-forward layer.\n",
    "            dropout: p of dropout.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim, n_heads=n_heads, dropout=dropout\n",
    "        )\n",
    "        self.joint_attention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim, n_heads=n_heads, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm_3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_hid_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        trg: torch.Tensor,\n",
    "        src: torch.Tensor,\n",
    "        trg_mask: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through architecture.\n",
    "\n",
    "        Args:\n",
    "            trg: Target tensor.\n",
    "            src: Source tensor.\n",
    "            trg_mask: Target tensor mask.\n",
    "            src_mask: Source tensor mask.\n",
    "        '''\n",
    "        trg_attention = self.attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.norm_1(trg + self.dropout(trg_attention))\n",
    "\n",
    "        joint_attention = self.attention(trg, src, src, src_mask)\n",
    "        trg = self.norm_2(trg + self.dropout(joint_attention))\n",
    "\n",
    "        out = self.mlp(trg)\n",
    "        out = self.norm_2(trg + self.dropout(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf028376",
   "metadata": {},
   "source": [
    "## Blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2bc49a",
   "metadata": {},
   "source": [
    "### `Encoder`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3733b4f",
   "metadata": {},
   "source": [
    "A classe `Encoder` segue a arquitetura abaixo:\n",
    "\n",
    "<center>\n",
    "<img src='assets/encoder.png' height=300/>\n",
    "</center>\n",
    "\n",
    "A camada recebe 8 parâmetros como entrada:\n",
    "\n",
    "- `vocab_size`: Tamanho do vocabulário de entrada.\n",
    "\n",
    "- `embed_dim`: Representa a dimensão total do embedding de cada token, que nada mais é do que o tamanho do vetor de características para cada token após a camada de embedding.\n",
    "\n",
    "- `n_layers`: Número de camadas de `EncoderLayer` empilhadas no modelo. Cada camada possibilita o modelo aprender com maior profundidade a capacidade de modelagem.\n",
    "\n",
    "- `n_heads`: Representa o número de cabeças de atenção. Cada _head_ é um mecanismo de atenção independente que processa diferentes subespaços de uma mesma entrada. Isso acontece devido a diferentes pesos em cada _head_.\n",
    "\n",
    "- `ff_hid_dim`: Representa o número de unidades da camada oculta dentro do bloco feed-forward do _encoder_.\n",
    "\n",
    "- `max_length`: Tamanho máximo da sequência de entrada suportada pelo modelo. É usada para criar embeddings posicionais.\n",
    "\n",
    "- `dropout`: Probabilidade _p_ de zerar um elemento do vetor de entrada.\n",
    "\n",
    "- `device`: Dispositivo de computação onde os tensores serão alocados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f91ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder Block with self-attention.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        ff_hid_dim: int,\n",
    "        max_length: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Class initializer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary.\n",
    "            embed_dim: Size of embedding dimension.\n",
    "            n_layers: Number of Encoder layers.\n",
    "            n_heads: Number of self-attention heads.\n",
    "            ff_hid_dim: Size of hidden dimension in feed-forward layer.\n",
    "            max_length: Maximum length of vector.\n",
    "            dropout: p of dropout.\n",
    "            device: Computing device\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.scale = embed_dim**0.5\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_length, embed_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    embed_dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    ff_hid_dim=ff_hid_dim,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through architecture.\n",
    "\n",
    "        Args:\n",
    "            src: Source tensor.\n",
    "            mask: Source tensor mask.\n",
    "        '''\n",
    "\n",
    "        N, seq_len = src.shape\n",
    "\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "        pos_embeddings = self.pos_emb(positions)\n",
    "        tok_embeddings = self.tok_emb(src) * self.scale\n",
    "        out = self.dropout(tok_embeddings + pos_embeddings)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            out = block(out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4596d",
   "metadata": {},
   "source": [
    "### `Decoder`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f9f81",
   "metadata": {},
   "source": [
    "A classe `Decoder` segue a arquitetura abaixo:\n",
    "\n",
    "<center>\n",
    "<img src='assets/decoder.png' height=300/>\n",
    "</center>\n",
    "\n",
    "A camada recebe 8 parâmetros como entrada:\n",
    "\n",
    "- `vocab_size`: Tamanho do vocabulário de entrada.\n",
    "\n",
    "- `embed_dim`: Representa a dimensão total do embedding de cada token, que nada mais é do que o tamanho do vetor de características para cada token após a camada de embedding.\n",
    "\n",
    "- `n_layers`: Número de camadas de `DecoderLayer` empilhadas no modelo. Cada camada possibilita o modelo aprender com maior profundidade a capacidade de modelagem.\n",
    "\n",
    "- `n_heads`: Representa o número de cabeças de atenção. Cada _head_ é um mecanismo de atenção independente que processa diferentes subespaços de uma mesma entrada. Isso acontece devido a diferentes pesos em cada _head_.\n",
    "\n",
    "- `ff_hid_dim`: Representa o número de unidades da camada oculta dentro do bloco feed-forward do _decoder_.\n",
    "\n",
    "- `max_length`: Tamanho máximo da sequência de entrada suportada pelo modelo. É usada para criar embeddings posicionais.\n",
    "\n",
    "- `dropout`: Probabilidade _p_ de zerar um elemento do vetor de entrada.\n",
    "\n",
    "- `device`: Dispositivo de computação onde os tensores serão alocados.\n",
    "\n",
    "A segunda camada de atenção recebe como _queries_ e _keys_, a saída da camada de encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ab94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decoder block with self-attention\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        ff_hid_dim: int,\n",
    "        max_length: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Class initializer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary.\n",
    "            embed_dim: Size of embedding dimension.\n",
    "            n_layers: Number of Encoder layers.\n",
    "            n_heads: Number of self-attention heads.\n",
    "            ff_hid_dim: Size of hidden dimension in feed-forward layer.\n",
    "            max_length: Maximum length of vector.\n",
    "            dropout: p of dropout.\n",
    "            device: Computing device\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.scale = embed_dim**0.5\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_length, embed_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    embed_dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    ff_hid_dim=ff_hid_dim,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        trg: torch.Tensor,\n",
    "        src: torch.Tensor,\n",
    "        trg_mask: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through architecture.\n",
    "\n",
    "        Args:\n",
    "            trg: Target tensor.\n",
    "            src: Source tensor.\n",
    "            trg_mask: Target tensor mask.\n",
    "            src_mask: Source tensor mask.\n",
    "        '''\n",
    "        N, trg_len = trg.shape\n",
    "\n",
    "        positions = torch.arange(0, trg_len).expand(N, trg_len).to(self.device)\n",
    "        pos_embeddings = self.pos_emb(positions)\n",
    "        tok_embeddings = self.tok_emb(trg) * self.scale\n",
    "        trg = self.dropout(tok_embeddings + pos_embeddings)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            trg = block(trg, src, trg_mask, src_mask)\n",
    "\n",
    "        out = self.fc(trg)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f3484",
   "metadata": {},
   "source": [
    "## Modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33899c7d",
   "metadata": {},
   "source": [
    "### `Transformer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f79b98",
   "metadata": {},
   "source": [
    "A classe `Transformer` segue a arquitetura abaixo:\n",
    "\n",
    "<center>\n",
    "<img src='assets/transformers_architecture.png' height=300/>\n",
    "</center>\n",
    "\n",
    "A camada recebe 11 parâmetros como entrada:\n",
    "\n",
    "- `src_vocab_size`: Tamanho do vocabulário da fonte.\n",
    "\n",
    "- `trg_vocab_size`: Tamanho do vocabulário do alvo.\n",
    "\n",
    "- `src_pad_idx`: Índice do token de padding da fonte.\n",
    "\n",
    "- `trg_pad_idx`: Índice do token de padding do alvo.\n",
    "\n",
    "- `embed_dim`: Representa a dimensão total do embedding de cada token, que nada mais é do que o tamanho do vetor de características para cada token após a camada de embedding.\n",
    "\n",
    "- `n_layers`: Número de camadas de `DecoderLayer` empilhadas no modelo. Cada camada possibilita o modelo aprender com maior profundidade a capacidade de modelagem.\n",
    "\n",
    "- `n_heads`: Representa o número de cabeças de atenção. Cada _head_ é um mecanismo de atenção independente que processa diferentes subespaços de uma mesma entrada. Isso acontece devido a diferentes pesos em cada _head_.\n",
    "\n",
    "- `ff_hid_dim`: Representa o número de unidades da camada oculta dentro do bloco feed-forward do _Decoder_.\n",
    "\n",
    "- `max_length`: Tamanho máximo da sequência de entrada suportada pelo modelo. É usada para criar embeddings posicionais.\n",
    "\n",
    "- `dropout`: Probabilidade _p_ de zerar um elemento do vetor de entrada.\n",
    "\n",
    "- `device`: Dispositivo de computação onde os tensores serão alocados.\n",
    "\n",
    "A classe também possui 2 métodos além do `forward`:\n",
    "\n",
    "- `src_mask`: Cria uma máscara de padding de um tensor fonte.\n",
    "\n",
    "- `trg_mask`: Cria uma look-ahead mask no tensor alvo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dafa42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Transformer architecture with Encoder and Decoder blocks.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        trg_vocab_size: int,\n",
    "        src_pad_idx: int,\n",
    "        trg_pad_idx: int,\n",
    "        embed_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        ff_hid_dim: int,\n",
    "        max_length: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Class initializer.\n",
    "\n",
    "        Args:\n",
    "            src_vocab_size: Size of source vocabulary.\n",
    "            trg_vocab_size: Size of target vocabulary.\n",
    "            src_pad_idx: Index of source padding token.\n",
    "            trg_pad_idx: Index of target padding token.\n",
    "            embed_dim: Size of embedding dimension.\n",
    "            n_layers: Number of Encoder layers.\n",
    "            n_heads: Number of self-attention heads.\n",
    "            ff_hid_dim: Size of hidden dimension in feed-forward layer.\n",
    "            max_length: Maximum length of vector.\n",
    "            dropout: p of dropout.\n",
    "            device: Computing device\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=src_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            ff_hid_dim=ff_hid_dim,\n",
    "            max_length=max_length,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=trg_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            ff_hid_dim=ff_hid_dim,\n",
    "            max_length=max_length,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def src_mask(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Creates mask for source tensor.\n",
    "        '''\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def trg_mask(self, trg: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Creates mask for target tensor.\n",
    "        '''\n",
    "        N, trg_len = trg.shape\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_pad_mask = (\n",
    "            torch.tril(torch.ones(trg_len, trg_len)).bool().to(self.device)\n",
    "            & trg_pad_mask\n",
    "        )\n",
    "\n",
    "        return trg_pad_mask\n",
    "\n",
    "    def forward(self, src: torch.Tensor, trg: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through architecture.\n",
    "\n",
    "        Args:\n",
    "            trg: Target tensor.\n",
    "            src: Source tensor.\n",
    "        '''\n",
    "        src_mask = self.src_mask(src)\n",
    "        trg_mask = self.trg_mask(trg)\n",
    "\n",
    "        encoded = self.encoder(src, src_mask)\n",
    "        decoded = self.decoder(trg, encoded, trg_mask, src_mask)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92cd4b",
   "metadata": {},
   "source": [
    "### `EncoderClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d423c",
   "metadata": {},
   "source": [
    "A classe recebe 10 parâmetros como entrada:\n",
    "\n",
    "- `vocab_size`: Tamanho do vocabulário da entrada.\n",
    "\n",
    "- `n_layers`: Número de camadas de `DecoderLayer` empilhadas no modelo. Cada camada possibilita o modelo aprender com maior profundidade a capacidade de modelagem.\n",
    "\n",
    "- `n_classes`: Número de classes a prever.\n",
    "\n",
    "- `embed_dim`: Representa a dimensão total do embedding de cada token, que nada mais é do que o tamanho do vetor de características para cada token após a camada de embedding.\n",
    "\n",
    "- `n_heads`: Representa o número de cabeças de atenção. Cada _head_ é um mecanismo de atenção independente que processa diferentes subespaços de uma mesma entrada. Isso acontece devido a diferentes pesos em cada _head_.\n",
    "\n",
    "- `ff_hid_dim`: Representa o número de unidades da camada oculta dentro do bloco feed-forward do _Decoder_.\n",
    "\n",
    "- `max_length`: Tamanho máximo da sequência de entrada suportada pelo modelo. É usada para criar embeddings posicionais.\n",
    "\n",
    "- `pad_idx`: Índice do token de padding da entrada.\n",
    "\n",
    "- `dropout`: Probabilidade _p_ de zerar um elemento do vetor de entrada.\n",
    "\n",
    "- `device`: Dispositivo de computação onde os tensores serão alocados.\n",
    "\n",
    "A classe também possui 1 método além do `forward`:\n",
    "\n",
    "- `mask`: Cria uma máscara de padding do tensor de entrada.\n",
    "\n",
    "Esse classificador se utiliza apenas da classe `Encoder`, adicionando ao fim uma camada completamente conectada para previsão das diferentes possíveis classes. O método forward cria a máscara da entrada, passa ela pelo encoder, e em seguida por uma camada de _dropout_.\n",
    "\n",
    "Após isso, é aplicado um _max-pooling_ à saída do _encoder_, redimensionando o tensor de $(batch\\ size \\times seq\\ len \\times embed\\ dim)$ para $(batch\\ size  \\times embed\\ dim)$. Finalmente a saída do _max-pooling_ passa pela camada completamente conectada, retornando os _logits_ (probabilidades) de cada classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e1177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    '''\n",
    "    Custom classifier with a Transformer's Encoder block.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_layers: int,\n",
    "        n_classes: int,\n",
    "        embed_dim: int,\n",
    "        n_heads: int,\n",
    "        ff_hid_dim: int,\n",
    "        max_length: int,\n",
    "        pad_idx: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Class initializer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary.\n",
    "            n_layers: Number of Encoder layers.\n",
    "            n_classes: Number of classes for output.\n",
    "            embed_dim: Size of embedding dimension.\n",
    "            n_heads: Number of self-attention heads.\n",
    "            ff_hid_dim: Size of hidden dimension in feed-forward layer of Encoder.\n",
    "            max_length: Maximum length of vector.\n",
    "            pad_idx: Index of padding token.\n",
    "            dropout: p of dropout.\n",
    "            device: Computing device\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            ff_hid_dim=ff_hid_dim,\n",
    "            max_length=max_length,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=n_classes,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.device = device\n",
    "\n",
    "    def mask(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''\n",
    "        Forward pass through architecture.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "        '''\n",
    "        mask = self.mask(x)\n",
    "\n",
    "        x = self.encoder(x, mask)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.max(dim=1)[0]\n",
    "\n",
    "        out = self.linear(x)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
